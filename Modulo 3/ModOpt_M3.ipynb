{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c42bd1cc-d2d1-4997-8e88-2b6a13d35470",
   "metadata": {},
   "source": [
    "# Modelado en Optimización (IIND-2501)\n",
    "## Módulo 3 - Tipos de problemas de optimización y estrategias de solución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41819d17-47e9-46d8-a2c3-d62eabe84734",
   "metadata": {},
   "source": [
    "En este módulo conoceremos:\n",
    "- *qué hacen internamente los solvers (de Excel, PuLP, u otros)* cuando resolvemos problemas de optimización, y\n",
    "- *qué hacen las herramientas de estadística y machine learning* cuando ajustan modelos de \"predicción\".\n",
    "\n",
    "Aunque se trata de problemas conceptualmente diferentes, ambos requieren procedimientos especializados de búsqueda para llegar a la mejor solución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75716b1c-dde3-4b42-bce1-871010a8cb7c",
   "metadata": {},
   "source": [
    "Usaremos la ecuación: \n",
    "$$\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\alpha \\Delta \\mathbf{x}$$\n",
    "como una forma general de representar diferentes procedimientos de búsqueda iterativa según las características de diferentes problemas de optimización. La dirección de movimiento $\\Delta \\mathbf{x}$ y tamaño del paso $\\alpha$ (que nos llevan de un punto $\\mathbf{x}_t$ al siguiente) vendrán determinados de diferentes formas según el tipo de problema y estrategia de solución: en casos como la programación lineal y la búsqueda de gradiente, los movimientos están basados en ecuaciones vectoriales, mientras que en procedimientos de búsqueda local y heurísticas, dichos movimientos se hacen mediante reglas algorítmicas acordes al tipo de problema.\n",
    "\n",
    "Durante este módulo, conoceremos estrategias de solución comunes para los principales problemas de optimización que aparecen en la práctica.\n",
    "\n",
    "1. Contrastaremos procedimientos basados en vecindarios (búsqueda local) y basados en derivadas (búsqueda de gradiente) para optimizar funciones no lineales sin restricciones, introduciendo nociones de *óptimo local/global*, *incumbente*, *convergencia* y *criterios de parada* (**Notebook 3.1**).\n",
    "   \n",
    "2. Estudiaremos el uso de métodos de gradiente para la optimización de modelos en el contexto de problemas de regresión, introduciendo la noción de función de pérdida como base para los problemas de estimación en *machine learning* e inteligencia artificial (**Notebook 3.2**).\n",
    "   \n",
    "3. Retomaremos los problemas de optimización lineal con restricciones (Módulos 1 y 2 del curso), introduciendo conceptos de *programación lineal* y presentando el método Simplex como estrategia de búsqueda vigente en la práctica y base de métodos avanzados. (**Notebooks 3.3 y 3.4**)\n",
    "   \n",
    "4. Introduciremos el uso de *solvers* especializados y heurísticas como alternativa práctica de solución para problemas de optimización complejos por su caracter combinatorio, o elementos no lineales (**Notebooks 3.5 y 3.6**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e76906-149c-4ae2-bf79-b23e91fa139f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
