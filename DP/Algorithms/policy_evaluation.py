def policy_evaluation(env, policy, gamma: float = 1.0, theta: float = 1e-6, report: bool = False):
    """
    ============================================================================
    EvaluaciÃ³n de una polÃ­tica  (determinista, entorno determinista)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Entradas
      â€¢ env      : entorno que implementa
                     â–¸ state_space()         â†’ Iterable[State]
                     â–¸ is_terminal(s)        â†’ bool
                     â–¸ sim_step(s, a)        â†’ (s', r)
      â€¢ policy   : dict[State, Action] â€” polÃ­tica determinista (Ï€(a|s)=ğŸ™{a=policy[s]})
      â€¢ gamma    : factor de descuento Î³ âˆˆ (0,1]
      â€¢ theta    : umbral de convergencia
      â€¢ report   : si es True, muestra trazas de cada iteraciÃ³n

    Salida
      â€¢ policy   : se devuelve la misma polÃ­tica recibida (conveniencia)
      â€¢ V        : dict[State, float] â€” aproximaciÃ³n de v_Ï€
    ============================================================================

    Notas
    -----
    * La funciÃ³n **no** modifica la polÃ­tica; simplemente la re-expone para que
      quien la llame pueda encadenar `policy, V = policy_evaluation(...)`
      sin perder la referencia.

    * El valor v_Ï€ se estima iterativamente usando la ecuaciÃ³n de Bellman
      para polÃ­ticas fijas:
            v(s) â† r + Î³Â·v(s')
      en entornos deterministas.
    """

    # -------------------------------------------------------------------------
    # 1. INICIALIZACIÃ“N: V(s) â† 0 para todos los estados
    # -------------------------------------------------------------------------
    V   = {s: 0.0 for s in env.state_space()}   # funciÃ³n valor inicial
    cnt = 1                                     # contador de iteraciones (solo para trazas)

    # -------------------------------------------------------------------------
    # 2. ITERACIÃ“N PRINCIPAL: aplicar Bellman hasta convergencia (|Î”| < Î¸)
    # -------------------------------------------------------------------------
    while True:

        if report:
            print(f"\nIteraciÃ³n {cnt}")

        delta = 0.0  # Î” â† 0

        # â”€â”€ 2.a Recorremos todos los estados del entorno
        for s in env.state_space():

            if env.is_terminal(s):        # omitimos estados terminales
                continue

            v = V[s]                      # valor anterior
            a = policy[s]                 # acciÃ³n segÃºn polÃ­tica Ï€
            next_s, r = env.sim_step(s, a)   # transiciÃ³n determinista: s â”€aâ†’ s'

            # â”€â”€ 2.b Actualizar valor de estado: Bellman para polÃ­ticas fijas
            new_v = r + gamma * V[next_s]
            V[s]  = new_v

            # â”€â”€ 2.c Actualizar mÃ¡ximo cambio observado
            delta = max(delta, abs(v - new_v))

            # â”€â”€ 2.d (Opcional) Mostrar trazas si hubo cambio
            if report and v != new_v:
                print(f"  s:{s} â”€a:{a}â†’ s':{next_s} | "
                      f"r={r:+.3f}, Î³V(s')={gamma*V[next_s]:+.3f} â†’ V(s)={new_v:+.3f}")

        # â”€â”€ 2.e Criterio de parada: convergencia si Î” < Î¸
        if delta < theta:
            break

        cnt += 1

    # -------------------------------------------------------------------------
    # 3. SALIDA: devolver V (y re-exponer la polÃ­tica para conveniencia)
    # -------------------------------------------------------------------------
    return V
