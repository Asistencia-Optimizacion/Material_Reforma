def policy_evaluation(env, policy, gamma: float = 1.0, theta: float = 1e-6, report: bool = False):
    """
    ============================================================================
    EvaluaciÃ³n de una polÃ­tica  (determinista, entorno determinista)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Entradas
      â€¢ env      : entorno que implementa
                     â–¸ state_space()         â†’ Iterable[State]
                     â–¸ is_terminal(s)        â†’ bool
                     â–¸ sim_step(s, a)        â†’ (s', r)
      â€¢ policy   : dict[State, Action] â€” polÃ­tica determinista (Ï€(a|s)=ğŸ™{a=policy[s]})
      â€¢ gamma    : factor de descuento Î³ âˆˆ (0,1]
      â€¢ theta    : umbral de convergencia
      â€¢ report   : si es True, muestra trazas de cada iteraciÃ³n

    Salida
      â€¢ policy   : se devuelve la misma polÃ­tica recibida (conveniencia)
      â€¢ V        : dict[State, float] â€” aproximaciÃ³n de v_Ï€
    ============================================================================

    Notas
    -----
    * La funciÃ³n **no** modifica la polÃ­tica; simplemente la re-expone para que
      quien la llame pueda encadenar `policy, V = policy_evaluation(...)`
      sin perder la referencia.
    """
    # 1. Inicializar V(s)=0 para todos los estados
    V   = {s: 0.0 for s in env.state_space()}
    cnt = 1  # contador de iteraciones (solo para â€˜reportâ€™)

    # 2. Iterar hasta la convergencia (Î” < Î¸)
    while True:

        if report:
            print(f"\nIteraciÃ³n {cnt}")

        delta = 0.0  # Î” â† 0

        # 2.a Recorremos cada estado s âˆˆ ğ’®
        for s in env.state_space():
            if env.is_terminal(s):          # omitimos terminales
                continue

            v      = V[s]                   # valor anterior
            a      = policy[s]              # acciÃ³n dictada por Ï€
            next_s, r = env.sim_step(s, a)  # transiciÃ³n Ãºnica (determinista)

            # Bellman: v(s) â† r + Î³Â·V(s')
            new_v = r + gamma * V[next_s]
            V[s]  = new_v

            # Actualizamos el mÃ¡ximo cambio Î”
            delta = max(delta, abs(v - new_v))

            if report and v != new_v:
                print(f"  s:{s} â”€a:{a}â†’ s':{next_s} | "
                      f"r={r:+.3f}, Î³V(s')={gamma*V[next_s]:+.3f} â†’ V(s)={new_v:+.3f}")

        if delta < theta:                   # criterio de parada
            break

        cnt += 1

    # 3. Devolver el valor estimado
    return V
