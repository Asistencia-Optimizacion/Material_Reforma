"""
# Aproxima V* directamente y luego deriva Ï€*
# Entradas:
#   S, A(s), p(s',r|s,a), Î³, Î¸
# Salidas:
#   Ï€* : polÃ­tica Ã³ptima
#   V* : funciÃ³n de valor Ã³ptima
# -------------------------------------------

# Fase 1: aproximar V*
V(s) â† 0  para todo s âˆˆ S

repetir
    Î” â† 0
    para cada s âˆˆ S:
        v â† V(s)
        V(s) â† max_a Î£_{s',r} p(s',r | s,a) Â· [ r + Î³ Â· V(s') ]
        Î” â† max(Î”, |v âˆ’ V(s)|)
hasta que Î” < Î¸

# Fase 2: derivar Ï€* usando V*
para cada s âˆˆ S:
    Ï€*(s) â† argmax_a Î£_{s',r} p(s',r | s,a) Â· [ r + Î³ Â· V(s') ]

return Ï€*, V
"""

def value_iteration(env, gamma=1.0, theta=1e-8):
    """
    ============================================================================
    Algoritmo : IteraciÃ³n de Valores (control Ã³ptimo, versiÃ³n determinista)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Entradas
      â€¢ env    : entorno que implementa
                   â–¸ state_space()         â†’ Iterable[State]
                   â–¸ is_terminal(s)        â†’ bool
                   â–¸ sim_step(s, a)        â†’ (s', r)
                   â–¸ actions(s)            â†’ Iterable[Action]
      â€¢ gamma  : factor de descuento Î³ âˆˆ [0,1]
      â€¢ theta  : umbral de convergencia Î¸ > 0

    Salida
      â€¢ policy : polÃ­tica Ã³ptima Ï€*
      â€¢ V      : valores Ã³ptimos v* asociados a Ï€*
    ============================================================================

    DescripciÃ³n resumida
    ---------------------------------------------------------------------------
      1.  Inicializa V(s) â† 0 para todo s âˆˆ ğ’®
      2.  Repite hasta convergencia:
            â–¸ V(s) â† max_a [ r + Î³Â·V(sâ€²) ]
            â–¸ Î” â† max_s |v_antiguo - V(s)|
      3.  Deriva Ï€*(s) â† argmax_a [ r + Î³Â·V(sâ€²) ]
      4.  Devuelve (Ï€*, V)
    """

    # -------------------------------------------------------------------------
    # 1. INICIALIZACIÃ“N: V(s) â† 0 para todos los estados
    # -------------------------------------------------------------------------
    V = {s: 0.0 for s in env.state_space()}

    # -------------------------------------------------------------------------
    # 2. ITERACIÃ“N DE VALORES: actualizar V hasta convergencia (Î” < Î¸)
    # -------------------------------------------------------------------------
    while True:
        delta = 0.0  # Î” â† 0 (mÃ¡ximo cambio en esta iteraciÃ³n)

        for s in env.state_space():
            if env.is_terminal(s):         # omitimos terminales
                continue

            # â”€â”€ 2.a Almacenar valor previo
            v = V[s]

            # â”€â”€ 2.b Calcular el mejor retorno esperado
            best_q = max(
                (env.sim_step(s, a)[1] + gamma * V[env.sim_step(s, a)[0]])
                for a in env.actions(s)
            )

            # â”€â”€ 2.c Actualizar V(s) â† best_q
            V[s] = best_q

            # â”€â”€ 2.d Actualizar el mÃ¡ximo cambio observado Î”
            delta = max(delta, abs(v - best_q))

        # â”€â”€ 2.e Criterio de parada
        if delta < theta:
            break

    # -------------------------------------------------------------------------
    # 3. DERIVAR POLÃTICA Ã“PTIMA Ï€*(s) â† argmax_a Q(s,a)
    # -------------------------------------------------------------------------
    policy = {}

    for s in env.state_space():
        if env.is_terminal(s):
            continue

        best_a, best_q = None, -float('inf')

        for a in env.actions(s):
            next_s, r = env.sim_step(s, a)
            q_sa = r + gamma * V[next_s]

            if q_sa > best_q:
                best_q, best_a = q_sa, a

        policy[s] = best_a

    # -------------------------------------------------------------------------
    # 4. SALIDA: polÃ­tica y funciÃ³n de valor Ã³ptimas
    # -------------------------------------------------------------------------
    return policy, V
