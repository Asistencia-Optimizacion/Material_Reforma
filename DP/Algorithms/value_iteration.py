def value_iteration(env, gamma=1.0, theta=1e-8):
    """
    ============================================================================
    Algoritmo : Iteraci√≥n de Valores (control √≥ptimo, versi√≥n determinista)
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Entradas
      ‚Ä¢ env    : entorno que implementa
                   ‚ñ∏ state_space()         ‚Üí Iterable[State]
                   ‚ñ∏ is_terminal(s)        ‚Üí bool
                   ‚ñ∏ sim_step(s, a)        ‚Üí (s', r)
                   ‚ñ∏ actions(s)            ‚Üí Iterable[Action]
      ‚Ä¢ gamma  : factor de descuento Œ≥ ‚àà [0,1]
      ‚Ä¢ theta  : umbral de convergencia Œ∏ > 0

    Salida
      ‚Ä¢ policy : pol√≠tica √≥ptima œÄ*
      ‚Ä¢ V      : valores √≥ptimos v* asociados a œÄ*
    ============================================================================

    Descripci√≥n resumida
    ---------------------------------------------------------------------------
      1.  Inicializa V(s) ‚Üê 0 para todo s ‚àà ùíÆ
      2.  Repite hasta convergencia:
            ‚ñ∏ V(s) ‚Üê max_a [ r + Œ≥¬∑V(s‚Ä≤) ]
            ‚ñ∏ Œî ‚Üê max_s |v_antiguo - V(s)|
      3.  Deriva œÄ*(s) ‚Üê argmax_a [ r + Œ≥¬∑V(s‚Ä≤) ]
      4.  Devuelve (œÄ*, V)
    """

    # -------------------------------------------------------------------------
    # 1. INICIALIZACI√ìN: V(s) ‚Üê 0 para todos los estados
    # -------------------------------------------------------------------------
    V = {s: 0.0 for s in env.state_space()}

    # -------------------------------------------------------------------------
    # 2. ITERACI√ìN DE VALORES: actualizar V hasta convergencia (Œî < Œ∏)
    # -------------------------------------------------------------------------
    while True:
        delta = 0.0  # Œî ‚Üê 0 (m√°ximo cambio en esta iteraci√≥n)

        for s in env.state_space():
            if env.is_terminal(s):         # omitimos terminales
                continue

            # ‚îÄ‚îÄ 2.a Almacenar valor previo
            v = V[s]

            # ‚îÄ‚îÄ 2.b Calcular el mejor retorno esperado
            best_q = max(
                (env.sim_step(s, a)[1] + gamma * V[env.sim_step(s, a)[0]])
                for a in env.actions(s)
            )

            # ‚îÄ‚îÄ 2.c Actualizar V(s) ‚Üê best_q
            V[s] = best_q

            # ‚îÄ‚îÄ 2.d Actualizar el m√°ximo cambio observado Œî
            delta = max(delta, abs(v - best_q))

        # ‚îÄ‚îÄ 2.e Criterio de parada
        if delta < theta:
            break

    # -------------------------------------------------------------------------
    # 3. DERIVAR POL√çTICA √ìPTIMA œÄ*(s) ‚Üê argmax_a Q(s,a)
    # -------------------------------------------------------------------------
    policy = {}

    for s in env.state_space():
        if env.is_terminal(s):
            continue

        best_a, best_q = None, -float('inf')

        for a in env.actions(s):
            next_s, r = env.sim_step(s, a)
            q_sa = r + gamma * V[next_s]

            if q_sa > best_q:
                best_q, best_a = q_sa, a

        policy[s] = best_a

    # -------------------------------------------------------------------------
    # 4. SALIDA: pol√≠tica y funci√≥n de valor √≥ptimas
    # -------------------------------------------------------------------------
    return policy, V
